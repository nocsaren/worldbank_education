---
title: "Exxample: Linear Model with Worldbank Data"
author: "Burak Can Serdar"
date: "`r Sys.Date()`"
output:
  html_document:
    css: "styles.css"
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
library(tidyverse)
library(ggplot2)
```
## Introduction
%explanation%

## Getting Data
#### GET function
We can use this function to get data from Worldbank API:

```{r getting, eval=FALSE}
library(tidyverse)
library(httr)
library(jsonlite)

get_specific_id <- function(country_1_code, indicator_code, number_values = 20000) {
  url <- paste0("https://api.worldbank.org/v2/country/", country_1_code, "/indicator/", indicator_code, "?format=json&per_page=", number_values) # forms an URL based on what we want in arguments
  
  result <- GET(url) # We use the URL to GET the result
  
  if (http_type(result) == "application/json") { # IF ordinary API response
    data <- content(result, as = "text") %>%
      fromJSON() %>%
      .[[2]] %>% # generally the juicy parts are at [2] 
      na.omit(value) %>% # cleaning rows with NA values 
      as_tibble()
    
    df_name_var <- paste0(indicator_code, "_", country_1_code) # dynamic dataframe name based on indicator and country
    assign(df_name_var, data, envir = .GlobalEnv) # assigns data to that dataframe and to environment
    saveRDS(data, file = paste0("./rds/", df_name_var, "_", Sys.Date(), ".rds")) # and outputs with a dynamic filename
    
  } else { # ELSE with erroneous API response
    error_msg <- content(result, as = "text") 
    cat("API Error:", error_msg, "\n")
    return(NULL)
  }
}
```

#### Finding Indicator Codes
It creates a file named ./rds/"indicator"\_"date".rds for later use. But how do we know which indicators are available? With this function:

```{r indicators, eval=FALSE}
library(tidyverse)
library(httr)
library(jsonlite)

id_list <- 
  GET("https://api.worldbank.org/v2/indicator?format=json&per_page=21020") %>%
  content(as = "text") %>%
  fromJSON() %>%
  .[[2]] %>%
  select(id, name, sourceNote) %>%  # sourceNote gives useful short explanations on indicators, better keep them
  rename(id_code = id, id_name = name, id_about = sourceNote) %>% # modifies to more useful column names
  select(id_code, id_name, id_about) %>%
  as_tibble()

save(id_list, 
     file = "id_list.rda") # outputs as rda
```

#### Searching through Indicators
It seems there are 21020 of them. We need a way to search to be able to quickly browse through them. This function searches through the names of indicators for maximum of 3 words bound with logical AND:

```{r search, eval=FALSE}
library(tidyverse)

search_id <- function(param_1, param_2 = NULL, param_3 = NULL) {
  load("id_list.rda")
  
  search_result <- id_list %>%
    filter(
      str_detect(id_name, regex(param_1, ignore_case = TRUE)) &
        if (!is.null(param_2)) str_detect(id_name, regex(param_2, ignore_case = TRUE)) else TRUE &
        if (!is.null(param_3)) str_detect(id_name, regex(param_3, ignore_case = TRUE)) else TRUE
    ) %>%
    select(id_code, id_name, id_about) %>%
    as_tibble()
  
  assign("search_result", search_result, envir = .GlobalEnv) # this creates a tibble named search_results on global environment
  
  return(search_result)
}
```

## Preparing Data
We chase our indicators PISA reading scores (LO.PISA.REA_all_2023-09-30.rds) and education expenditures (SE.XPD.TOTL.GD.ZS_all_2023-09-30.rds), got them, and saved them. 

We could do all this dynamically, getting data in real time from the API, but it will cause unnecessary delay in each run of our code and put unnecessary load on remote server. Data we are getting is not that dynamic anyway, so it makes no sense to check it for updates in each run. 

Now that we have the rds files, we will import them to R as dataframes:

```{r startup pisa}
pisa_results <- readRDS("./rds/LO.PISA.REA_all_2023-09-30.rds") # assigning pisa as dataframe
```

PISA dataframe looks like this:
```{r startup_pisa_prev, echo=FALSE}
head(pisa_results)
```
 
Do the same to expenditures:
```{r startup expenditure}
expenditure <- readRDS("./rds/SE.XPD.TOTL.GD.ZS_all_2023-09-30.rds") # assigning expenditures as dataframe 
```

And expenditures look like this: 
```{r startup_expenditure_prev, echo=FALSE}
head(expenditure)
```
#### Join
Now, we have our pisa_results and expenditure dataframes, we will prepare these data for our analysis. A quick glance shows us that some country-date data overlaps in both dataframes, some do not. We will only use countries and dates that exist in bpth data, therefore we have to do an inner join . Before that, some fine tuning in our dataframes like changing the col names so that we will know which columns is which after the join:

```{r cleanup}
cleaned_pisa <- pisa_results %>% # preparing the dataframe for join
  mutate(country = countryiso3code) %>% 
  rename(value_pisa = value) %>% 
  select(country, date, value_pisa)
  
cleaned_exp <- expenditure %>% # preparing the dataframe for join
  mutate(country = countryiso3code) %>% 
  rename(value_exp = value) %>% 
  select(country, date, value_exp)
joined <- inner_join(cleaned_pisa, cleaned_exp, by = c("country", "date")) # inner joining the two dataframes by their "country" and "date" column

rm(list = c("pisa_results", "expenditure", "cleaned_pisa", "cleaned_exp")) # we won't need all those dataframes we created anymore.
```
The dataframe joined looks like this:
``` {r joined, echo=FALSE}
head(joined)
```
## Msaffasfasfasanic

asfasasfasfsaffsa

```{r regress}

model <- lm(value_pisa ~value_exp, joined) # This is the part we actually create our model. we ask if "value_pisa" values depend on "value_exp" values in the "joined" dataframe. 
summary(model)

ggplot(data = joined, aes(x = value_exp, y = value_pisa)) +
  geom_point(mapping = aes(color = country)) +
  geom_smooth(method = "lm", se = TRUE) +
  guides(color = "none")+
  labs(
    title = "Expenditure of Education (%) vs. PISA Reading Scores",
    x = "Expenditure",
    y = "PISA Reading Scores"
  )

```

asfasfas

sfafasasf
